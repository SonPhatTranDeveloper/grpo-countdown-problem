# Fast training configuration for quick experiments

# Learning parameters
learning_rate: 1e-5
weight_decay: 0.01
warmup_ratio: 0.1
lr_scheduler_type: linear
optim: adamw_8bit

# Training parameters
num_train_epochs: 1
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
bf16: true
remove_unused_columns: false

# Generation parameters
temperature: 0.8
max_completion_length: 256
num_generations: 8
max_prompt_length: 2048

# Logging and saving
report_to: 
  - tensorboard
logging_steps: 5
save_strategy: steps
save_steps: 25

# Reward function weights (optional)
reward_weights:
  format_reward: 1.0
  arithmetic_format_reward: 1.0
  correctness_reward: 1.0
